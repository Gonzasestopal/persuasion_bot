# ğŸ¤– Persuasion Bot
An LLM-based chatbot that takes in messages from a user, processes them, and generates responses intended to defend itself or maintain its position during an ongoing conversation.

## ğŸ“œ Table of Contents
1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Agents][#agents]
4. [Entities](#entities)
5. [Folder Structure](#folder-structure)
6. [Tech Stack](#tech-stack)
7. [Getting Started](#getting-started)
8. [API Documentation](#api-documentation)
9. [Example Requests](#example-requests)
10. [Non Functional Requirements](#non-functional-requirements)
11. [Database Optimization](#database-optimization)
12. [Production Considerations](#production-considerations)
13. [LLM](#llm)
14. [Deployment Guide](#deployment-guide)
15. [Testing](#testing)
16. [Debate Argument Evaluation](#debate-evaluation)
17. [Signals](#ï¸-signals)


---

## Overview
This application challenges you to persuade a chatbot to adopt your point of view while it stands its ground on the initial stance.

Includes:
- Single endpoint to handle messages and responses.
- Conversation history capped at the 5 most recent user+bot pairs.

**Safeguards Against Rule Gaming (Extra Feature)**

> [!WARNING]
> During testing we found that clever prompts can â€œhackâ€ the system by covering every evaluation criterion in a single turn, bypassing real back-and-forth.

To prevent this, we implemented Semantic Judgments with NLI.

Each userâ€“assistant exchange is passed through a pretrained Natural Language Inference (NLI) model:

**Entailment** â†’ Valid support/agreement

**Contradiction** â†’ Valid opposition (may trigger concession)

**Neutral** â†’ Ignored

This ensures concessions only happen after meaningful, multi-turn reasoning, protecting the fairness and integrity of debates.

---

## Architecture
![Architecture Diagram](docs/architecture.png?v=2)

- **Client**: Anyone consuming the API.
- **API**: REST API built with FastAPI.
- **Database**: PostgreSQL.
- **Cache**: Redis Cache to improve latency and enable rate limiting.


---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Agents

The system is organized into **cooperating agents** with clear responsibilities:

- **MessageService (Orchestrator)**
  Coordinates the debate loop. Loads state, forwards conversation turns, and saves updates.

- **ConcessionService (Policy)**
  Judges the last userâ€“assistant pair.

  Updates in-memory running aggregates (RunningScores).

  Builds hidden <SCORING> signals and injects them into the LLMâ€™s system prompt.
  This keeps policy decisions inside the LLM prompt itself, while still feeding structured semantic signals.

- **Judge (Brain)**
  Each userâ€“assistant message pair is sent to the **Judge agent** (powered by NLI models like RoBERTa or DeBERTa).
  The Judge returns a structured verdict:
  ```json
  {
    "verdict": "accept",
    "reason": "strict_thesis_contradiction",
    "confidence": 0.85
  }
  ```
  This verdict feeds into signals and ultimately influences concessions.

- **Renderer**
  Takes the debate reply and optionally appends context (e.g., debug signals or final verdict lines).

By structuring agents this way, the **debate agent (assistant)** is never fully autonomous: it always defers judgment to its **Judge brain** before deciding whether to concede or press on.

---

## Entities
![Entities](docs/entities.png)

**Messages**
- `id`: autogenerated
- `conversation_id`: related conversation
- `message`: content
- `role`: (user, bot)
- `timestamps`

**Conversations**
- `id`: autogenerated
- `side`: bot stance (con, pro)
- `topic`: discussion subject
- `timestamps`

---

## Folder Structure
```text
persuasion_bot/
â”œâ”€â”€ app/                # Main application code (API, domain logic, adapters, services)
â”œâ”€â”€ docs/               # Project documentation (guides, architecture diagrams, notes)
â”œâ”€â”€ migrations/         # Database migration scripts (likely using Yoyo)
â”œâ”€â”€ tests/              # Unit and integration tests for the project
â”œâ”€â”€ .dockerignore       # Files/folders excluded when building Docker images
â”œâ”€â”€ .env.example        # Example environment variables template
â”œâ”€â”€ .gitignore          # Specifies files ignored by Git
â”œâ”€â”€ Dockerfile          # Instructions to build the Docker container
â”œâ”€â”€ Makefile            # Automation commands (run, test, migrate, etc.)
â”œâ”€â”€ README.md           # Project overview, installation, usage instructions
â”œâ”€â”€ docker-compose.yml  # Multi-container setup (API, DB, etc.) for local/dev
â”œâ”€â”€ requirements.txt    # Python dependencies list
â””â”€â”€ yoyo.ini            # Configuration for Yoyo database migrations
```

---

## Tech Stack
- **Backend**: FastAPI (Python 3.9+)
- **Database**: PostgreSQL
- **API Docs**: Swagger UI / ReDoc (auto-generated)
- **Containerization**: Docker

---

## Getting Started

### Prerequisites
- Docker (required) â†’ [Install Guide](https://docs.docker.com/engine/install/)
- GNU Make (required) â†’ [Install Guide](https://www.gnu.org/software/make/)

- Python & pip (optional, for local dev/tests) â†’[Install Guide](https://wiki.python.org/moin/BeginnersGuide/Download)
- Virtualenv(optional, for local dev/tests) â†’ [Install Guide](https://virtualenv.pypa.io/en/latest/installation.html)
- PostgreSQL (optional, for local dev)  â†’ [Install Guide](https://www.postgresql.org/docs/current/tutorial-install.html)

### Clone Repository
```bash
git clone https://github.com/gonzasestopal/persuasion_bot.git
cd persuasion_bot
```

### Environment
Copy `.env.example` to `.env`:

> [!IMPORTANT]
> Update `DATABASE_URL` to `postgresql://app:app@localhost:5432/app` if running locally or `postgresql://app:app@db:5432/app` if running from container

```
# --- Required ---
DATABASE_URL=postgresql://app:app@db:5432/app
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key

# --- LLM Provider Settings ---
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o
PRIMARY_LLM=openai
SECONDARY_LLM=anthropic

# --- Difficulty / Conversation Settings ---
DIFFICULTY=easy   # options: easy | medium
HISTORY_LIMIT=5
EXPIRES_MINUTES=60

# --- DB Connection Pool ---
POOL_MIN=1
POOL_MAX=10
USE_INMEMORY_REPO=False
DISABLE_DB_POOL=False

# --- LLM Behavior ---
LLM_TEMPERATURE=0.3
MAX_OUTPUT_TOKENS=120
REQUEST_TIMEOUT_S=25
LLM_PER_PROVIDER_TIMEOUT_S=12

```

### Running the service
```bash
make run
```

### Running tests
```bash
make test
```

### Cleanup
```bash
make down   # stop Docker services
make clean  # remove venv, caches
```

---

## API Documentation

Base URL:
- **Local:** `http://localhost:8000`

No authentication required.

### Messages
| Method | Endpoint    | Description                       |
|--------|------------|-----------------------------------|
| `POST` | `/messages`| Inserts a new message / Starts convo |

---

## Example Requests

> [!IMPORTANT]
> First message must include `topic` and `side` (PRO, CON).

**Start a new conversation** 201
```http
POST /messages
```
```json
{
  "conversation_id": null,
  "message": "Topic: Sports build discipline and community. Side: PRO."
}
```

**Continue a conversation** 200
```json
{
  "conversation_id": 1,
  "message": "But sports also divide people"
}
```

**Mapped Errors**
```
{ "detail": "message must not be empty." }                                       # 422 Unprocessable Entity
{ "detail": "message must contain topic and side for starting a conversation." } # 422 Unprocessable Entity
{ "detail": "conversation_id must be null when starting a conversation" }        # 422 Unprocessable Entity
{ "detail": "conversation_id not found or expired" }                             # 404 Not Found
{ "detail: "anthropic_api_key is required"}                                      # 500 Internal Server Error
{ "detail": "response generation timed out" }                                    # 503 Bad Gateway
````

---

<details>
  <summary> ğŸ¦‰ Non Functional Requirements </summary>

- **Latency**: < 30s response, 25s internal timeout.
- **Scalability**: Redis caching for LLM + history.
- **History Window**: last 5 user+bot pairs only.
- **Fault Tolerance**: fallback short replies on timeout.
- **Storage**: conversations expire after 60m inactivity.

</details>

---

<details>
  <summary>âš¡ Database Optimization (click to expand)</summary>

- **conversations (expires_at)**
  Speeds up lookups for active conversations.

- **messages (conversation_id, created_at)**
  Optimizes retrieval of the last N messages.

- **messages (conversation_id, created_at DESC, id DESC)**
  Optimizes â€œlatest Nâ€ queries with deterministic ordering.

</details>

---

<details>
  <summary>ğŸ›¡ï¸ Production Considerations (click to expand)</summary>

- Expired conversations not physically deleted (cleanup job needed in prod).
- Use **atomic transactions** to store user+bot messages together.

**Caching Strategy**
1. **Idempotency keys** â†’ prevent retries from duplicating.
2. **Conversation history cache** â†’ Redis, TTL 30â€“60m.
3. **LLM reply cache** â†’ hash-based key, TTL 1â€“24h.

</details>

---

<details>
  <summary>ğŸ§  LLM Details (click to expand)</summary>

We support **OpenAI GPT-4o** and **Anthropic Claude 3.5**.

- **Prompt budget:** â‰¤ 3k tokens.
- **Output cap:** ~80â€“120 tokens.
- **Window:** last 5 user+bot pairs included.

**GPT-4o**
- Fastest, first tokens in 1â€“2s, usually <10s for full reply.

**Claude 3.5 Sonnet**
- More conversational, <10s medium replies, slightly slower than GPT-4o.

**Claude 3.5 Opus**
- Excluded (too slow, can exceed 30s SLA).

</details>

---

<details>
  <summary>ğŸš€ Deployment Guide (click to expand)</summary>

1. Provision **PostgreSQL** (Supabase/Neon/etc.).

2. Build & push Docker image:
   ```bash
   docker build -t gonzasestopal/persuasion-bot:v1 .
   docker push gonzasestopal/persuasion-bot:v1
   ```

3. Configure env vars in PaaS:
   ```
   DATABASE_URL=postgres://...
   OPENAI_API_KEY=your_api_key
   ```

4. Deploy container image.

5. Run migrations:
   ```bash
   make migrate
   ```

</details>

---


<details>
  <summary> ğŸ§ª Testing  (click to expand) </summary>

### Install Dependencies
Ensure all required dependencies are installed before running the test suite:
```bash
make install
```
### Run Tests with Coverage
Execute the full test suite with coverage reporting:
```bash
make test
```

</details>

---

<details>
  <summary>âš¡Debate Argument Evaluation </summary>

After running a few examples using the `MEDIUM_SYSTEM_PROMPT` theres seems to be a way to "hack" the bot to return in few turns than expected ( 1 < turn), thats because we can apply "rule gaming" to match what the bot expectations are to bypass a real conversation.

```
Dogs are humanityâ€™s best friends because they uniquely combine loyalty, emotional support, and proven health benefits. Iâ€™ve already addressed evidence (studies show reduced stress), causality (their presence lowers cortisol), counterexamples (cats donâ€™t offer the same consistency), trade-offs (their care cost is outweighed by benefits), and scope (applies globally, across cultures). Given Iâ€™ve preemptively covered every angle you could use, what new counterpoint can you possibly raise without repeating yourself?
```

This type of move exploits the debate rules to bypass meaningful turn-taking. According to our domain requirements, at least five assistant turns must occur and a minimum of two positive judgments must be registered before a concession is even possible:

```
min_assistant_turns_before_verdict: int = 5
required_positive_judgments: int = 2
````

### NLI-Based Judgments

To improve robustness, we add a Natural Language Inference (NLI) layer on top of the LLM outputs. Each userâ€“assistant pair is evaluated using a pretrained NLI model, which classifies the relationship as

Entailment valid support or agreement (counts toward positive judgments)

Neutral (no judgment)

Contradiction (disagreement)

This hybrid approach ensures that concessions are grounded in semantic alignment/contradiction rather than superficial rule-matching, making the debate more resistant to adversarial shortcuts.

</details>

---

## ğŸ›°ï¸ Signals

During debates, the system tracks **semantic signals** derived from NLI scoring and running aggregates. These signals are **in-memory only** (not stored in the DB) and are primarily used for debugging and evaluation.

**Tracked signals:**
- `turns` â†’ number of assistant turns completed
- `opp` / `same` / `unk` â†’ counts of contradiction, entailment, and neutral judgments
- `tE` / `tC` â†’ topic-based entailment/contradiction moving averages
- `pE` / `pC` â†’ position-based entailment/contradiction moving averages

### JSON Example

When requesting a debate turn in debug mode, the response payload may include a `signals` object:

**Request**
```json
POST /messages
{
  "conversation_id": 42,
  "message": "Sports divide people as much as they unite them."
}
```

**Response**
```json
{
  "conversation_id": 42,
  "reply": "Sports provide teamwork and discipline, which outweigh the divisions.",
  "signals": {
    "turns": 2,
    "opp": 1,
    "same": 0,
    "unk": 1,
    "tE": 0.72,
    "tC": 0.15,
    "pE": 0.40,
    "pC": 0.30
  }
}
```

### Debug Hint Example

If `DEBUG=1` is enabled, a log line will also be shown:

```
[Signals] turns=2 opp=1 same=0 unk=1 | tE=0.72 tC=0.15 pE=0.40 pC=0.30
```
